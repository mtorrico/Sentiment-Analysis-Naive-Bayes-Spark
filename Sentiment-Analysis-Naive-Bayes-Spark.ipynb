{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initiate a SparkSession. A SparkSession initializes both a SparkContext and a SQLContext to use RDD-based and DataFrame-based functionalities of Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark as ps\n",
    "spark = (ps.sql.SparkSession.builder \n",
    "        .master(\"local[4]\") \n",
    "        .appName(\"sparkSQL exercise\") \n",
    "        .getOrCreate()\n",
    "        )\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis using Naive Bayes\n",
    "Here we are going to create a text indexing pipeline for user reviews based on Spark ML library. We check the structure of that dataframe, and the column detected in the json content, by using .printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- asin: string (nullable = true)\n",
      " |-- helpful: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      " |-- overall: double (nullable = true)\n",
      " |-- reviewText: string (nullable = true)\n",
      " |-- reviewTime: string (nullable = true)\n",
      " |-- reviewerID: string (nullable = true)\n",
      " |-- reviewerName: string (nullable = true)\n",
      " |-- summary: string (nullable = true)\n",
      " |-- unixReviewTime: long (nullable = true)\n",
      "\n",
      "line count: 10261\n"
     ]
    }
   ],
   "source": [
    "# read json\n",
    "reviews_df = spark.read.json('data/data/reviews_Musical_Instruments_5.json.gz')\n",
    "\n",
    "# prints the schema\n",
    "reviews_df.printSchema()\n",
    "\n",
    "# some functions are still valid\n",
    "print(\"line count: {}\".format(reviews_df.count()))\n",
    "\n",
    "# show the table in a oh-so-nice format\n",
    "#yelp_business_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will keep only the columns reviewText and overall. We can check our transformation to verify we are getting what we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = reviews_df.select(['reviewText', 'overall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- reviewText: string (nullable = true)\n",
      " |-- overall: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+\n",
      "|          reviewText|overall|\n",
      "+--------------------+-------+\n",
      "|Not much to write...|    5.0|\n",
      "|The product does ...|    5.0|\n",
      "|The primary job o...|    5.0|\n",
      "|Nice windscreen p...|    5.0|\n",
      "|This pop filter i...|    5.0|\n",
      "|So good that I bo...|    5.0|\n",
      "|I have used monst...|    5.0|\n",
      "|I now use this ca...|    3.0|\n",
      "|Perfect for my Ep...|    5.0|\n",
      "|Monster makes the...|    5.0|\n",
      "|Monster makes a w...|    5.0|\n",
      "|I got it to have ...|    4.0|\n",
      "|If you are not us...|    3.0|\n",
      "|I love it, I used...|    5.0|\n",
      "|I bought this to ...|    5.0|\n",
      "|I bought this to ...|    2.0|\n",
      "|This Fender cable...|    4.0|\n",
      "|wanted it just on...|    5.0|\n",
      "|I've been using t...|    5.0|\n",
      "|Fender cords look...|    5.0|\n",
      "+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a label for classification and a balanced dataset\n",
    "- This dataset is made of user reviews and ratings:\n",
    "    - The reviewText column (string) contains the raw text of the review.\n",
    "    - The overall column (double) contains the rating given by the user, in {1.0, 2.0, 3.0, 4.0, 5.0}\n",
    "    - We are going to focus on extreme ratings {1.0, 5.0} which will be refer to as the negative and positive class respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|overall|count(reviewText)|\n",
      "+-------+-----------------+\n",
      "|    5.0|             6938|\n",
      "|    4.0|             2084|\n",
      "|    3.0|              772|\n",
      "|    2.0|              250|\n",
      "|    1.0|              217|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "df_out = (new_df.groupBy(\"overall\")\n",
    "                  .agg(F.count(\"reviewText\"))\n",
    "                  .orderBy(\"overall\", ascending=False)\n",
    "         )\n",
    "\n",
    "df_out.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A rating of 5 has a count of 6938 which is 30 times more than the count for a rating of 1. Hence to have a balanced set, we need each rating to have 217 samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process to balance data\n",
    "- We will create two dataframes:\n",
    "    - One for the reviews having an overall of 1.0 (we will call them the neg class),\n",
    "    - Another for the reviews having an overall of 5.0 (we will call them the pos class).\n",
    "    - We will limit the number of reviews in each dataframe by the number we identified previously of 217 and shufle before applying the limit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+\n",
      "|          reviewText|overall|\n",
      "+--------------------+-------+\n",
      "|Not much to write...|    5.0|\n",
      "|The product does ...|    5.0|\n",
      "|The primary job o...|    5.0|\n",
      "|Nice windscreen p...|    5.0|\n",
      "|This pop filter i...|    5.0|\n",
      "|So good that I bo...|    5.0|\n",
      "|I have used monst...|    5.0|\n",
      "|Perfect for my Ep...|    5.0|\n",
      "|Monster makes the...|    5.0|\n",
      "|Monster makes a w...|    5.0|\n",
      "|I love it, I used...|    5.0|\n",
      "|I bought this to ...|    5.0|\n",
      "|wanted it just on...|    5.0|\n",
      "|I've been using t...|    5.0|\n",
      "|Fender cords look...|    5.0|\n",
      "|The Fender 18 Fee...|    5.0|\n",
      "|Got this cable to...|    5.0|\n",
      "|When I was search...|    5.0|\n",
      "|The ends of the m...|    5.0|\n",
      "|Just trying to fi...|    5.0|\n",
      "+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pos = new_df.filter(new_df.overall == 5)\n",
    "df_pos.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+--------------------+\n",
      "|          reviewText|overall|               randn|\n",
      "+--------------------+-------+--------------------+\n",
      "|Not much to write...|    5.0|  0.4085363219031828|\n",
      "|The product does ...|    5.0|  0.8811793095417685|\n",
      "|The primary job o...|    5.0|  -2.013921870967947|\n",
      "|Nice windscreen p...|    5.0|  1.6641751435679302|\n",
      "|This pop filter i...|    5.0| -1.0878600404148453|\n",
      "|So good that I bo...|    5.0|  1.1432831717404852|\n",
      "|I have used monst...|    5.0| -0.1668332100007998|\n",
      "|Perfect for my Ep...|    5.0|  0.9728134830024971|\n",
      "|Monster makes the...|    5.0| -1.8922625416463206|\n",
      "|Monster makes a w...|    5.0|  -1.406958171706003|\n",
      "|I love it, I used...|    5.0|  0.5598396336190025|\n",
      "|I bought this to ...|    5.0| 0.25154049516128324|\n",
      "|wanted it just on...|    5.0| -1.0231123693572317|\n",
      "|I've been using t...|    5.0| -0.5507468559455683|\n",
      "|Fender cords look...|    5.0|    2.80044811525585|\n",
      "|The Fender 18 Fee...|    5.0| -0.4441804987714544|\n",
      "|Got this cable to...|    5.0| -1.1877333201721334|\n",
      "|When I was search...|    5.0|-0.03323488317203...|\n",
      "|The ends of the m...|    5.0|  1.3655627017360528|\n",
      "|Just trying to fi...|    5.0| 0.47633982920307877|\n",
      "+--------------------+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import randn\n",
    "df_temp = df_pos.withColumn('randn', randn(seed=42))\n",
    "df_temp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+------------------+\n",
      "|          reviewText|overall|             randn|\n",
      "+--------------------+-------+------------------+\n",
      "|I bought one for ...|    5.0| 3.605892795878719|\n",
      "|Exactly what I ne...|    5.0|3.5847812457770263|\n",
      "|I don't have a lo...|    5.0| 3.485952639155781|\n",
      "|In my house alone...|    5.0|3.4816327449385627|\n",
      "|Other than that, ...|    5.0|3.2466669637174808|\n",
      "|After reading num...|    5.0| 3.234441808486619|\n",
      "|I bought this to ...|    5.0| 3.228942703491076|\n",
      "|it is lined and h...|    5.0| 3.211540930369533|\n",
      "|I am self taught,...|    5.0|3.1805432487134673|\n",
      "|These are guitar ...|    5.0| 3.160484834634083|\n",
      "|I bought a tusq n...|    5.0|3.1086052096584713|\n",
      "|Very nice product...|    5.0|3.0697456406156816|\n",
      "|I have used Fende...|    5.0| 3.067345236139749|\n",
      "|I've been playing...|    5.0| 3.062071141776958|\n",
      "|Super sturdy, wel...|    5.0| 3.058307421766882|\n",
      "|JF-15:again, OMFG...|    5.0| 3.057082631122537|\n",
      "|It definitely cha...|    5.0|2.9939447504471213|\n",
      "|This is a very us...|    5.0|2.9447514403607875|\n",
      "|I like mic stands...|    5.0|2.9384161066628622|\n",
      "|I've bought quite...|    5.0|2.8996152808292877|\n",
      "+--------------------+-------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_217_pos = df_temp.orderBy(df_temp.randn.desc()).limit(217)\n",
    "df_217_pos.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_217_pos = df_217_pos.select(['reviewText', 'overall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+\n",
      "|          reviewText|overall|\n",
      "+--------------------+-------+\n",
      "|I bought one for ...|    5.0|\n",
      "|Exactly what I ne...|    5.0|\n",
      "|I don't have a lo...|    5.0|\n",
      "|In my house alone...|    5.0|\n",
      "|Other than that, ...|    5.0|\n",
      "|After reading num...|    5.0|\n",
      "|I bought this to ...|    5.0|\n",
      "|it is lined and h...|    5.0|\n",
      "|I am self taught,...|    5.0|\n",
      "|These are guitar ...|    5.0|\n",
      "|I bought a tusq n...|    5.0|\n",
      "|Very nice product...|    5.0|\n",
      "|I have used Fende...|    5.0|\n",
      "|I've been playing...|    5.0|\n",
      "|Super sturdy, wel...|    5.0|\n",
      "|JF-15:again, OMFG...|    5.0|\n",
      "|It definitely cha...|    5.0|\n",
      "|This is a very us...|    5.0|\n",
      "|I like mic stands...|    5.0|\n",
      "|I've bought quite...|    5.0|\n",
      "+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pos_neg_df = df_217_pos.union(df_neg)\n",
    "pos_neg_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|overall|count(reviewText)|\n",
      "+-------+-----------------+\n",
      "|    5.0|              217|\n",
      "|    1.0|              217|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_out = (pos_neg_df.groupBy(\"overall\")\n",
    "                  .agg(F.count(\"reviewText\"))\n",
    "                  .orderBy(\"overall\", ascending=False)\n",
    "         )\n",
    "\n",
    "df_out.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schema shows we have balanced data set correctly for the positive and negative class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+-----+\n",
      "|          reviewText|overall|label|\n",
      "+--------------------+-------+-----+\n",
      "|I bought one for ...|    5.0|  1.0|\n",
      "|Exactly what I ne...|    5.0|  1.0|\n",
      "|I don't have a lo...|    5.0|  1.0|\n",
      "|In my house alone...|    5.0|  1.0|\n",
      "|Other than that, ...|    5.0|  1.0|\n",
      "|After reading num...|    5.0|  1.0|\n",
      "|I bought this to ...|    5.0|  1.0|\n",
      "|it is lined and h...|    5.0|  1.0|\n",
      "|I am self taught,...|    5.0|  1.0|\n",
      "|These are guitar ...|    5.0|  1.0|\n",
      "|I bought a tusq n...|    5.0|  1.0|\n",
      "|Very nice product...|    5.0|  1.0|\n",
      "|I have used Fende...|    5.0|  1.0|\n",
      "|I've been playing...|    5.0|  1.0|\n",
      "|Super sturdy, wel...|    5.0|  1.0|\n",
      "|JF-15:again, OMFG...|    5.0|  1.0|\n",
      "|It definitely cha...|    5.0|  1.0|\n",
      "|This is a very us...|    5.0|  1.0|\n",
      "|I like mic stands...|    5.0|  1.0|\n",
      "|I've bought quite...|    5.0|  1.0|\n",
      "+--------------------+-------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType, FloatType\n",
    "\n",
    "def classifier(x):\n",
    "    if x == 1.0:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return 1.0\n",
    "    \n",
    "my_class_udf = udf(classifier, DoubleType())\n",
    "merge_df = pos_neg_df.withColumn('label', my_class_udf(pos_neg_df['overall']))\n",
    "merge_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- reviewText: string (nullable = true)\n",
      " |-- overall: double (nullable = true)\n",
      " |-- label: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "merge_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define fucntions for data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wordlist=['cabl', 'b', 'good', 'qualiti', 'guitar', 'price', 'cord', 'hosa', 'great', 'end']\n"
     ]
    }
   ],
   "source": [
    "import pyspark as ps    # for the pyspark suite\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "import string\n",
    "import unicodedata\n",
    "\n",
    "import nltk\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.util import ngrams\n",
    "from nltk import pos_tag\n",
    "from nltk import RegexpParser\n",
    "\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.feature import IDF\n",
    "\n",
    "import sys\n",
    "\n",
    "def extract_bow_from_raw_text(text_as_string):\n",
    "    \"\"\"Extracts bag-of-words from a raw text string.\n",
    "    Parameters\n",
    "    ----------\n",
    "    text (str): a text document given as a string\n",
    "    Returns\n",
    "    -------\n",
    "    list : the list of the tokens extracted and filtered from the text\n",
    "    \"\"\"\n",
    "    if (text_as_string == None):\n",
    "        return []\n",
    "\n",
    "    if (len(text_as_string) < 1):\n",
    "        return []\n",
    "\n",
    "    if sys.version_info[0] < 3:\n",
    "        nfkd_form = unicodedata.normalize('NFKD', unicode(text_as_string))\n",
    "    else:\n",
    "        nfkd_form = unicodedata.normalize('NFKD', str(text_as_string))\n",
    "\n",
    "    text_input = str(nfkd_form.encode('ASCII', 'ignore'))\n",
    "\n",
    "    sent_tokens = sent_tokenize(text_input)\n",
    "\n",
    "    tokens = list(map(word_tokenize, sent_tokens))\n",
    "\n",
    "    sent_tags = list(map(pos_tag, tokens))\n",
    "\n",
    "    grammar = r\"\"\"\n",
    "        SENT: {<(J|N).*>}                # chunk sequences of proper nouns\n",
    "    \"\"\"\n",
    "\n",
    "    cp = RegexpParser(grammar)\n",
    "    ret_tokens = list()\n",
    "    stemmer_snowball = SnowballStemmer('english')\n",
    "\n",
    "    for sent in sent_tags:\n",
    "        tree = cp.parse(sent)\n",
    "        for subtree in tree.subtrees():\n",
    "            if subtree.label() == 'SENT':\n",
    "                t_tokenlist = [tpos[0].lower() for tpos in subtree.leaves()]\n",
    "                t_tokens_stemsnowball = list(map(stemmer_snowball.stem, t_tokenlist))\n",
    "                #t_token = \"-\".join(t_tokens_stemsnowball)\n",
    "                #ret_tokens.append(t_token)\n",
    "                ret_tokens.extend(t_tokens_stemsnowball)\n",
    "            #if subtree.label() == 'V2V': print(subtree)\n",
    "    #tokens_lower = [map(string.lower, sent) for sent in tokens]\n",
    "\n",
    "    return(ret_tokens)\n",
    "\n",
    "\n",
    "def indexing_pipeline(input_df, **kwargs):\n",
    "    \"\"\"Runs a full text indexing pipeline on a collection of texts contained in a DataFrame.\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_df (DataFrame): a DataFrame that contains a field called 'text'\n",
    "    Returns\n",
    "    -------\n",
    "    df : the same DataFrames with a column called 'features' for each document\n",
    "    wordlist : the list of words in the vocabulary with their corresponding IDF\n",
    "    \"\"\"\n",
    "    inputCol_ = kwargs.get(\"inputCol\", \"text\")\n",
    "    vocabSize_ = kwargs.get(\"vocabSize\", 5000)\n",
    "    minDF_ = kwargs.get(\"minDF\", 2.0)\n",
    "\n",
    "    # ugly: to add that to our slave nodes so that it finds the bootstrapped nltk_data\n",
    "    nltk.data.path.append('/home/hadoop/nltk_data')\n",
    "\n",
    "    extract_bow_from_raw_text(\"\")  # ugly: for instanciating all dependencies of this function\n",
    "    tokenizer_udf = udf(extract_bow_from_raw_text, ArrayType(StringType()))\n",
    "    df_tokens = input_df.withColumn(\"bow\", tokenizer_udf(col(inputCol_)))\n",
    "\n",
    "    cv = CountVectorizer(inputCol=\"bow\", outputCol=\"vector_tf\", vocabSize=vocabSize_, minDF=minDF_)\n",
    "    cv_model = cv.fit(df_tokens)\n",
    "    df_features_tf = cv_model.transform(df_tokens)\n",
    "\n",
    "    idf = IDF(inputCol=\"vector_tf\", outputCol=\"features\")\n",
    "    idfModel = idf.fit(df_features_tf)\n",
    "    df_features = idfModel.transform(df_features_tf)\n",
    "\n",
    "    return(df_features, cv_model.vocabulary)\n",
    "\n",
    "\n",
    "if (__name__ == \"__main__\"):\n",
    "    spark = ps.sql.SparkSession.builder \\\n",
    "                .master(\"local[4]\") \\\n",
    "                .appName(\"df lecture\") \\\n",
    "                .getOrCreate()\n",
    "\n",
    "    dfMusical = spark.read.json('data/data/reviews_Musical_Instruments_5.json.gz')\n",
    "    df = dfMusical.select('reviewText', 'overall').withColumnRenamed(\"reviewText\", \"text\").limit(100)\n",
    "\n",
    "    df, wordlist = indexing_pipeline(df)\n",
    "\n",
    "    print(\"wordlist={}\".format(wordlist[0:10]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we apply function to our previous DataFrame to index every review as well as obtain the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+-----+--------------------+--------------------+--------------------+\n",
      "|          reviewText|overall|label|                 bow|           vector_tf|            features|\n",
      "+--------------------+-------+-----+--------------------+--------------------+--------------------+\n",
      "|I bought one for ...|    5.0|  1.0|[b, tenor, differ...|(1281,[1,7,11,19,...|(1281,[1,7,11,19,...|\n",
      "|Exactly what I ne...|    5.0|  1.0|[guitar, home, sa...|(1281,[0,21,32,19...|(1281,[0,21,32,19...|\n",
      "|I don't have a lo...|    5.0|  1.0|[b, lot, experi, ...|(1281,[1,2,4,6,7,...|(1281,[1,2,4,6,7,...|\n",
      "|In my house alone...|    5.0|  1.0|[hous, bass, play...|(1281,[0,7,11,18,...|(1281,[0,7,11,18,...|\n",
      "|Other than that, ...|    5.0|  1.0|[10-32, rack, scr...|(1281,[65,91,142,...|(1281,[65,91,142,...|\n",
      "|After reading num...|    5.0|  1.0|[b, numer, review...|(1281,[0,1,4,6,8,...|(1281,[0,1,4,6,8,...|\n",
      "|I bought this to ...|    5.0|  1.0|[b, axess, mp1504...|(1281,[1,4,16,53,...|(1281,[1,4,16,53,...|\n",
      "|it is lined and h...|    5.0|  1.0|[b'it, pouch, son...|(1281,[5,10,12,34...|(1281,[5,10,12,34...|\n",
      "|I am self taught,...|    5.0|  1.0|[b, self, taught,...|(1281,[1,75,83,18...|(1281,[1,75,83,18...|\n",
      "|These are guitar ...|    5.0|  1.0|[b, guitar, pick,...|(1281,[0,1,15,27,...|(1281,[0,1,15,27,...|\n",
      "|I bought a tusq n...|    5.0|  1.0|[b, tusq, nut, tu...|(1281,[0,1,4,6,10...|(1281,[0,1,4,6,10...|\n",
      "|Very nice product...|    5.0|  1.0|[b'veri, nice, pr...|(1281,[3,4,6,12,3...|(1281,[3,4,6,12,3...|\n",
      "|I have used Fende...|    5.0|  1.0|[b, fender, thin,...|(1281,[1,5,21,25,...|(1281,[1,5,21,25,...|\n",
      "|I've been playing...|    5.0|  1.0|[b, i\\, year, gui...|(1281,[0,1,3,4,6,...|(1281,[0,1,3,4,6,...|\n",
      "|Super sturdy, wel...|    5.0|  1.0|[b, super, sturdi...|(1281,[1,18,227,5...|(1281,[1,18,227,5...|\n",
      "|JF-15:again, OMFG...|    5.0|  1.0|[b, jf-15, mk, ty...|(1281,[0,1,2,6,7,...|(1281,[0,1,2,6,7,...|\n",
      "|It definitely cha...|    5.0|  1.0|[b'it, sound, cla...|(1281,[0,6,33,51,...|(1281,[0,6,33,51,...|\n",
      "|This is a very us...|    5.0|  1.0|[b, use, littl, d...|(1281,[0,1,3,36,5...|(1281,[0,1,3,36,5...|\n",
      "|I like mic stands...|    5.0|  1.0|[b, mic, stand, l...|(1281,[1,10,14,18...|(1281,[1,10,14,18...|\n",
      "|I've bought quite...|    5.0|  1.0|[b, few, tuner, y...|(1281,[1,5,11,22,...|(1281,[1,5,11,22,...|\n",
      "+--------------------+-------+-----+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(index_df, vocab) = indexing_pipeline(merge_df,inputCol='reviewText')\n",
    "index_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary: ['guitar', 'b', 'pedal', 'string', 'good', 'great', 'sound', 'other', 'amp', 'time']\n"
     ]
    }
   ],
   "source": [
    "print(\"vocabulary: {}\".format(vocab[0:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying the Naive Bayes algorithm for sentiment analysis\n",
    "- The basics of ML pipelining in spark relies on building step by step instances of classes drawn from the pyspark.ml library. We will now use the class:\n",
    "    - pyspark.ml.classification.NaiveBayes : implements the Naive Bayes algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|          reviewText|overall|label|                 bow|           vector_tf|            features|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|*Incredible quali...|    5.0|  1.0|[b'*incred, quali...|(1281,[12,14,16,3...|(1281,[12,14,16,3...|[-700.22958515685...|[0.99999266163137...|       0.0|\n",
      "|-  This tuner is ...|    5.0|  1.0|[b, tuner, afford...|(1281,[0,1,5,7,11...|(1281,[0,1,5,7,11...|[-1433.8085621809...|[0.99999999847579...|       0.0|\n",
      "|A little nicer th...|    5.0|  1.0|[b, littl, nicer,...|(1281,[1,17,27,36...|(1281,[1,17,27,36...|[-229.78352598638...|[8.18921424978878...|       1.0|\n",
      "|After reading num...|    5.0|  1.0|[b, numer, review...|(1281,[0,1,4,6,8,...|(1281,[0,1,4,6,8,...|[-1746.3499402273...|[2.50740589007966...|       1.0|\n",
      "|After researching...|    5.0|  1.0|[b, overdr, pedal...|(1281,[0,1,2,5,9,...|(1281,[0,1,2,5,9,...|[-1295.0440450766...|[5.98133874630392...|       1.0|\n",
      "|As some of the ot...|    5.0|  1.0|[b, other, review...|(1281,[0,1,7,12,2...|(1281,[0,1,7,12,2...|[-292.42696266398...|[1.95109713411677...|       1.0|\n",
      "|Beautiful pedal, ...|    5.0|  1.0|[b'beauti, pedal,...|(1281,[2,10,35,56...|(1281,[2,10,35,56...|[-337.43239719079...|[3.69829349927765...|       1.0|\n",
      "|Behringer pedals ...|    5.0|  1.0|[b, behring, peda...|(1281,[0,1,2,10,1...|(1281,[0,1,2,10,1...|[-626.33832054108...|[5.84250711504327...|       1.0|\n",
      "|Bought a couple o...|    5.0|  1.0|[b'bought, coupl,...|(1281,[35,91,92,1...|(1281,[35,91,92,1...|[-349.68361962234...|[1.92533550291960...|       1.0|\n",
      "|D'Addario strings...|    5.0|  1.0|[b, d'addario, st...|(1281,[0,1,3,4,16...|(1281,[0,1,3,4,16...|[-214.62053127434...|[2.29877888182801...|       1.0|\n",
      "|Fantastic sound -...|    5.0|  1.0|[b'fantast, sound...|(1281,[3,6,31,167...|(1281,[3,6,31,167...|[-250.46279070899...|[3.30483702188472...|       1.0|\n",
      "|First of all, a l...|    5.0|  1.0|[b'first, littl, ...|(1281,[0,5,12,21,...|(1281,[0,5,12,21,...|[-1183.9143449512...|[0.99995834547890...|       0.0|\n",
      "|For years I've ow...|    5.0|  1.0|[b'for, year, i\\,...|(1281,[4,5,10,12,...|(1281,[4,5,10,12,...|[-1011.6020379884...|[0.68518471458166...|       0.0|\n",
      "|Good little conne...|    5.0|  1.0|[b'good, littl, c...|(1281,[14,36,43,4...|(1281,[14,36,43,4...|[-162.60002513461...|[0.85160136814381...|       0.0|\n",
      "|Great little tune...|    5.0|  1.0|[b'great, littl, ...|(1281,[5,11,36,38...|(1281,[5,11,36,38...|[-338.71099500115...|[5.37828118433177...|       1.0|\n",
      "|Great replacement...|    5.0|  1.0|[b'great, replac,...|(1281,[3,5,34,124...|(1281,[3,5,34,124...|[-154.04628600222...|[8.02974114158410...|       1.0|\n",
      "|Guitarists have b...|    5.0|  1.0|[b'guitarist, bun...|(1281,[0,2,4,5,6,...|(1281,[0,2,4,5,6,...|[-3523.0473593133...|[1.48948161067055...|       1.0|\n",
      "|I am self taught,...|    5.0|  1.0|[b, self, taught,...|(1281,[1,75,83,18...|(1281,[1,75,83,18...|[-454.85723018232...|[2.30872020236670...|       1.0|\n",
      "|I bought one for ...|    5.0|  1.0|[b, tenor, differ...|(1281,[1,7,11,19,...|(1281,[1,7,11,19,...|[-539.24731973743...|[1.31197724180213...|       1.0|\n",
      "|I bought these be...|    5.0|  1.0|[b, mike, cord, s...|(1281,[1,4,10,35,...|(1281,[1,4,10,35,...|[-763.17557346817...|[0.09358853584684...|       1.0|\n",
      "+--------------------+-------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "splits = index_df.randomSplit([0.7, 0.3], 24)\n",
    "merge_df_train = splits[0]\n",
    "merge_df_test = splits[1]\n",
    "\n",
    "# create the trainer and set its parameters\n",
    "nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\")\n",
    "\n",
    "# train the model\n",
    "nf_fit = nb.fit(merge_df_train)\n",
    "\n",
    "# apply the model on the test set\n",
    "trans_test_df = nf_fit.transform(merge_df_test)\n",
    "\n",
    "trans_test_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the MulticlassClassificationEvaluator to obtain an evaluation of the accuracy of your classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7647058823529411"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator.evaluate(trans_test_df)\n",
    "evaluator.evaluate(trans_test_df, {evaluator.metricName: \"accuracy\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation of the NaiveBayes results\n",
    "- The NaiveBayes model provides an internal matrix model.theta that we can convert into a numpy array with model.theta.toArray(). This matrix contains two columns corresponding to the two classes: 0 for negative and 1 for positive\n",
    "\n",
    "- The values inside that matrix correspond, for each class, to the prior probabilities used to compute the likelihood of a document to belong to the class. In this implementation, the model.theta matrix doesn't provide probabilities, but log of probabilities\n",
    "\n",
    "- We use this model.theta matrix, combined with the vocabulary obtained above from CountVectorizer, to obtain words that are related to the positive class, and words that are related to the negative class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1281)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = nf_fit.theta.toArray()\n",
    "idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-5.00133423, -5.79306503, -5.26479709, ..., -7.8855047 ,\n",
       "        -7.8855047 , -7.8855047 ],\n",
       "       [-4.70935861, -5.74273032, -4.33579478, ..., -7.63164235,\n",
       "        -7.63164235, -9.41951657]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['guitar', 'b', 'pedal', 'string', 'good', 'great', 'sound',\n",
       "       'price', 'tuner', 'more', 'differ', 'fender', 'tone', 'mic',\n",
       "       'year', 'pick', 'capo', 'better', 'case', 'nice', 'littl', 'one',\n",
       "       'lot', 'set', 'mani', 'last', 'small', 'box', 'unit', 'use',\n",
       "       'light', 'bit', 'fret', 'effect', 'music', 'bass', 'end', 'day',\n",
       "       'batteri', 'sure', 'easi', 'high', 'perfect', 'bag', 'line',\n",
       "       'power', 'peopl', 'best', 'electr', 'screw', 'right', 'gig',\n",
       "       'boss', 'size', 'most', 'adjust', 'place', 'board', 'sever',\n",
       "       'heavi', 'level', 'type', 'joyo', 'microphon', \"b'this\", 'strat',\n",
       "       'hand', 'expens', 'cool', 'fact', 'solid', 'onli', 'clean',\n",
       "       'weight', 'color', 'origin', 'player', 'finger', 'full', 'long',\n",
       "       'stock', 'practic', 'stuff', 'overdr', 'audio', 'ukulel', 'anyon',\n",
       "       'gain', 'behring', 'big', 'interfac', 'accur', 'hour', 'erni',\n",
       "       'home', 'everyth', 'side', 'bright', 'happi', 'simpl', 'store',\n",
       "       'featur', 'strong', 'thick', 'spot', 'clip', 'equip', 'black',\n",
       "       'huge', 'rock', 'delay', 'musician', 'sturdi', 'reverb', 'portabl',\n",
       "       'drive', 'blue', 'gaug', 'speaker', 'amaz', 'violin', 'mixer',\n",
       "       'vintag', 'perform', 'medium', 'comput', 'impress', 'inexpens',\n",
       "       'need', 'knob', 'stage', 'switch', 'studio', 'excel', 'while',\n",
       "       'crunch', 'pad', 'similar', 'job', 'ultim', \"d'addario\", 'system',\n",
       "       'lower', 'pro', 'singl', 'larger', 'that\\\\', 'valu', 'rang',\n",
       "       'inch', 'les', 'paul', 'smaller', 'past', 'care', 'boutiqu',\n",
       "       'manufactur', 'kid', 'test', 'touch', 'snark', 'look', 'headstock',\n",
       "       'get', 'amount', 'friend', 'mandolin', 'deal', 'squier', 'mount',\n",
       "       'sustain', 'overal', 'favorit', 'dream', 'peg', 'comfort',\n",
       "       'flexibl', 'space', 'basic', 'pickguard', 'fullton', 'delux',\n",
       "       'grip', 'classic', 'easier', 'seri', 'american', 'pa', 'pleas',\n",
       "       'hous', 'buzz', 'lighter', 'keyboard', 'option', 'tremolo', 'buy',\n",
       "       'analog', 'vibrat', \"b'these\", 'sensit', 'tubescream',\n",
       "       'profession', 'song', 'marshal', 'fantast', 'boom', 'doubl',\n",
       "       'wireless', 'love', 'rich', 'cleaner', 'smooth', 'tech', 'enough',\n",
       "       'nut', 'room', 'yamaha', 'clear', ']', 'mid', 'compon', 'surpris',\n",
       "       'life', 'sweet', 'recommend', 'action', 'stratocast', 'adapt',\n",
       "       'older', 'handi', 'exact', 'mouth', 'fall', 'finish', 'white',\n",
       "       'headphon', 'fun', 'od', 'bronz', 'rack', 'book', 'super', 'minor',\n",
       "       'lesson', 'ocd', 'etc', 'plenti', 'afford', 'fast', 'pretti',\n",
       "       'm-audio', 'saddl', 'duti', 'breakup', 'hot', 'pc', 'gear', 'solo',\n",
       "       'stomp', 'anchor', 'track', 'eas', 'sorri', 'dead', 'cat', 'dial',\n",
       "       'iv', 'start', 'chromat', 'soprano', 'tabl', 'hanger', 'chain',\n",
       "       'volt', 'ds-1', 'page', 'jf-02', 'trebl', 'shure', 'be', 'ts',\n",
       "       'preamp', 'son', 'cant', 'caus', 'veri', 'usual', 'futur', 'dirt',\n",
       "       'mix', 'christma', 'slight', 'dano', 'compact', 'tinni', 'gretsch',\n",
       "       'budget', 'speed', 'glad', 'telecast', 'phosphor', \"b'great\",\n",
       "       'stabl', 'attack', 'word', 'thicker', 'conveni', 'll',\n",
       "       'comparison', 'rig', 'yesterday', 'suppli', 'beginn', 'clone',\n",
       "       'import', 'hardwar', 'output', 'bend', 'tip', 'front', 'gibson',\n",
       "       'thinner', 'charact', 'jr', 'us', 'heavier', 'rhythm', 'beauti',\n",
       "       'pair', 'project', 'zoom', 'nylon', \"b'veri\", 'factor', 'tweak',\n",
       "       'includ', 'protect', 'program', 'state', 'california', 'drum',\n",
       "       'neat', 'g7', 'celest', 'combin', 'rehears', 'tele', 'ac',\n",
       "       'warranti', 'vox', 'danelectro', 'wow', 'truss', 'church',\n",
       "       'vendor', 'unbeliev', 'entir', 'clariti', 'plate', 'haha',\n",
       "       'they\\\\', 'requir', 'concern', 'altern', 'make', 'wonder', 'hesit',\n",
       "       'cloth', 'forev', 'turn', \"didn\\\\'t\", 'readi', 'pen', 'engin',\n",
       "       'rat', 'rid', 'applic', 'sheet', \"d\\\\'addario\", 'jazzmast', '.73',\n",
       "       '*', 'quick', 'fretboard', 'spec', 'cheapi', 'titl', 'wast',\n",
       "       'elixir', 'babi', 'venu', 'polyweb', 'date', 'corros', 'ibanez',\n",
       "       'pain', 'detail', 'versatil', 'taylor', 'show', 'enjoy', 'immedi',\n",
       "       'epiphon', 'entri', 'muddi', 'winder', 'leg', 'notic',\n",
       "       'particular', 'construct', 'spring', 'fresh', 'transpar', 'precis',\n",
       "       'tutori', 'present', 'modern', 'incred', 'reissu', 'techniqu',\n",
       "       'wear', 'rod', 'dark', 'free', 'distanc', 'reson', 'chang', 'safe',\n",
       "       'definit', '[', 'non', 'relat', 'stainless', 'boat', 'build',\n",
       "       'boy', 'semi', 'tradit', 'quiet', 'hundr', 'polish', 'fan',\n",
       "       'height', 'bolt', 'dual', 'factori', 'man', 'chrome', 'echo',\n",
       "       'file', 'prize', 'plus', 'video', 'mild', 'footswitch', 'condens',\n",
       "       'tough', 'imho', 'delic', 'utter', 'em', 'heavy-duti', 'tweed',\n",
       "       'except', 'convert', 'distinct', 'adequ', 'alesi', 'booster',\n",
       "       'clearer', 'daughter', 'difference.th', 'drill', 'strength', 'ad',\n",
       "       'contact', 'banjo', 'flavor', 'pearl', 'creativ', 'pound', 'will',\n",
       "       'background', 'nadi', 'serious', 'got', 'two', 'final', 'mod',\n",
       "       'label', 'load', 'depth', 'iii', '.i', 'objection', 'tighter',\n",
       "       'hell', 'mile', 'placement', 'btw', 'esp', 'bracket', 'bluegrass',\n",
       "       'built-in', 'awesom', 'moment', 'form', 'clock', 'quest', 'round',\n",
       "       'lp', 'epi', 'pyle', \"b'bought\", 'self', 'stereo', 'pressur',\n",
       "       'leader', 'dulcim', 'guitar\\\\', 'circuitri', 'processor',\n",
       "       'graviti', 'yep', 'situat', 'mass', 'dirtbox', 'visual', 'imposs',\n",
       "       'mid-rang', 'nicer', 'substanti', 'metronom', 'uca202', 'meet',\n",
       "       'cleanish', 'proco', 'carri', 'and', 'camp', 'valv', 'louder', 'u',\n",
       "       'worri', 'happier', 'satisfi', 'essenti', 'abil', 'my', 'harmoni',\n",
       "       'accept', 'area', 'tear', 'eq', 'deep', 'lightweight',\n",
       "       \"shouldn\\\\'t\", 'histori', 'session', 'swivel', 'step', 'closer',\n",
       "       'ca', 'fyi', 'residu', 'u.s.', 'heck', 'pic', 'gadget', 'bullet',\n",
       "       'general', 'discern', 'prone', 'femal', 'harmonica', 'bundl',\n",
       "       'spectacular', 'act', 'pup', 'hitch', 'typic', 'oper', 'monster',\n",
       "       'market', 'bank', 'fabric', \"b'one\", 'give', 'decad', 'spaghetti',\n",
       "       'blend', 'dynam', 'print', 'harmon', 'though', 'hollow', 'kinda',\n",
       "       'honest', 'expect', 'clutter', 'trip', \"b'mi\", 'brighter', 'net',\n",
       "       'silver', 'youtub', 'smell', 'improv', 'snap', 'british',\n",
       "       'pantera', 'amazon.com', 'extens', 'method', 'equal', 'run',\n",
       "       'center', 'multi', 'school', 'aggress', 'tast', 'just', 'sonic',\n",
       "       'ts9', 'slinki', 'diamet', 'prefer', 'suppos', 'shini', 'steal',\n",
       "       'drummer', 'concert', 'acoustic/electr', 'mellow', 'retail',\n",
       "       'hump', 'vs', 'dava', 'mass-produc', 'spare', 'trem', 'what\\\\',\n",
       "       'slide', 'jazz', 'endless', 'scratchi', 'sm58', 'drywal',\n",
       "       'well-mad', 'mobil', 'anyway', 'invest', 'variat', 'voic',\n",
       "       'crapshoot', 'abus', \"isn\\\\'t\", 'pouch', 'mode', 'instal',\n",
       "       'legaci', 'comment', 'allow', 'mexico', 'invalu'], dtype='<U15')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "vocab_arr = np.array(vocab)\n",
    "vocab_arr[idx[1,:] > idx[0,:]]    #vocab for positve class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most words are associated with a positive feeling which aligns with our assumption that a 5 star reviews is positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['other', 'amp', 'time', 'product', 'thing', 'cabl', 'qualiti',\n",
       "       'strap', 'stand', 'problem', 'way', 'review', 'few', 'i\\\\',\n",
       "       'cheap', 'same', 'acoust', 'someth', 'new', 'distort', 'money',\n",
       "       'issu', 'note', 'volum', 'neck', 'much', 'amazon', 'instrument',\n",
       "       'i', 'record', 'piec', 'first', 'work', 'low', 'tune', 'metal',\n",
       "       'tube', 'week', 'bad', 'brand', 'ball', 'it\\\\', 'fine', 'plug',\n",
       "       'noth', 'cord', 'pickup', 'item', 'nois', 'plastic', 'less',\n",
       "       'standard', 'model', 'part', 'packag', 'hard', 'kind', 'planet',\n",
       "       'bottom', 'seller', 'star', 'month', 'mine', 'old', 'anyth',\n",
       "       'idea', 'next', \"b'the\", 'bridg', 'buck', 'top', 'real', 'play',\n",
       "       'purchas', 'bodi', 'design', 'feel', 'second', 'wrong', 'button',\n",
       "       'own', 'point', 'junk', 'back', 'wave', 'not', 'defect', 'style',\n",
       "       'input', 'extra', 'chord', 'head', 'dollar', 'useless', 'dunlop',\n",
       "       'jack', 'minut', 'posit', 'disappoint', 'e', 'signal', 'abl',\n",
       "       'cost', 'result', 'hole', 'reason', 'worth', 'short', 'matter',\n",
       "       'replac', 'tri', 'channel', 'rogu', 'coupl', 'refund', 'crap',\n",
       "       'thank', 'rubber', 'live', \"doesn\\\\'t\", 'poor', 'respons', 'order',\n",
       "       'guitarist', 'base', 'cover', 'compani', 'wire', 'a', 'support',\n",
       "       'main', 'such', 'connect', 'garbag', 'everyon', 'descript',\n",
       "       'version', 'whole', 'experi', 'least', 'control', 'name', 'red',\n",
       "       'guy', 'direct', 'bar', 'flat', 'connector', 'pack', 'monitor',\n",
       "       'materi', 'decent', 'steel', 'hiss', 'devic', 'drop', 'shop',\n",
       "       'possibl', 'return', 'durabl', \"b'it\", 'band', 'lock', 'thin',\n",
       "       'special', 'mean', 'usb', 'setup', 'servic', 'choic', 'xlr',\n",
       "       'soft', 'rate', 'screen', 'harp', 'stay', 'custom', 'handl',\n",
       "       'focusrit', 'hassl', 'digit', 'you\\\\', 'flimsi', 'loop', 'display',\n",
       "       'driver', 'ok', 'like', 'tight', 'cours', 'ear', 'total',\n",
       "       'opinion', 'arm', 'softwar', 'wall', 'folk', 'middl', 'thread',\n",
       "       'watt', 'damag', 'electron', 'stiff', 'shoulder', 'digitech',\n",
       "       'velcro', 'key', 'unwant', 'feedback', 'weird', 'someon', 'the',\n",
       "       'go', 'movement', 'ground', 'person', 'insid', 'wors', 'china',\n",
       "       'ii', 'it.i', 'loud', 'function', 'feet', 'angl', 'regular',\n",
       "       'cheaper', 'updat', 'true', 'night', 'green', 'reliabl', 'tool',\n",
       "       'pop', 'your', 'terribl', 'websit', 'wheel', 'chromacast',\n",
       "       'pocket', 'intern', 'email', 'dri', 'chanc', 'worst', 'korg',\n",
       "       'midi', 'reput', 'read', 'critic', 'filter', 'fit', 'major', 'it',\n",
       "       'wide', 'g', 'pot', 'martin', 'loos', 'open', 'uke', 'longer',\n",
       "       'today', 'mistak', 'akai', 'oil', 'frequenc', 'compressor', 'ton',\n",
       "       'warn', 'close', 'okay', 'patch', 'garageband', 'pitch', 'receiv',\n",
       "       'fair', 'parti', 'ship', 'luck', 'to', 'frustrat', 'local',\n",
       "       'pictur', 'leather', '/', 'phase', 'length', 'unus', 'avail',\n",
       "       'consist', 'dull', 'aw', 'jam', 'shimmer', 'awar', 'lack', 'octav',\n",
       "       'solut', 'face', 'mustang', 'clamp', 'sharp', 'noisi', 'this',\n",
       "       'bypass', 'floor', 'world', 'realiti', 'industri', 'doe', 'larg',\n",
       "       'dr', 'zipper', 'needl', 'bewar', 'buyer', 'tortex', 'reaper',\n",
       "       'come', 'weld', \"don\\\\'t\", 'mind', 'varieti', 'mini', 'ipad',\n",
       "       'broken', 'higher', 'but', 'swell', 'process', 'shoe', '%', \"b'ok\",\n",
       "       'seagul', 'third', 'crazi', 'crappi', 'repair', 'activ', 'pin',\n",
       "       'help', 'd', 'v', 'faith', 'hum', 'natur', 'chines', 'difficult',\n",
       "       'goos', 'block', 'onlin', 'holder', 'section', 'lead', 'darn',\n",
       "       'secur', 'fuse', 'fade', 'actual', 'axe', 'none', 'phone',\n",
       "       'complaint', 'find', 'surfac', 'clayton', 'term', 'gooseneck',\n",
       "       'mike', 'master', 'cheer', 'gap', 'thought', 'famili', 'bear',\n",
       "       'roll', 'extend', 'error', 'member', 'hde', 'peavey', 'addit',\n",
       "       'here\\\\', 'dud', 'rout', 'continu', 'friction', 'forum', 'mac',\n",
       "       'lemon', 'forth', 'half', 'exampl', 'plug-in', 'limit', 'in',\n",
       "       'proper', 'popular', 'memori', 'newbi', 'is', 'roland', 'hear',\n",
       "       'forc', 'slow', 'eye', 'mxr', 'narrow', 'excus', 'tascam',\n",
       "       'stupid', 'collect', 'dirti', 'set-up', 't', 'occas', 'site',\n",
       "       'attach', 'multipl', 'adhes', 'manual', 'obvious', 'warm', 'orang',\n",
       "       'usabl', 'foot', 'bummer', 'sale', 'tab', 'up', 'lip', 'outlet',\n",
       "       'fix', 'potenti', 'suggest', 'outstand', 'surround', 'iphon',\n",
       "       'coil', 'vocal', 'capabl', 'male', 'attempt', 'meter', 'bulki',\n",
       "       'worthless', 'crackl', 'stronger', 'mess', 'regardless', 'dozen',\n",
       "       'left', 'water', 'negat', 'awhil', 'all', 'you', 'c', 'technic',\n",
       "       'crash', 'wtf', 'ns', 'godaw', 'send', 'arriv', 'phenomenon.w',\n",
       "       'horribl', 'caveat', 'fizz', \"b'just\", 'profit', 'tension',\n",
       "       'search', 'cheapli', 'interest', 'unabl', 'miracl', 'spray',\n",
       "       'advic', \"b'unless\", 'irig', 'weekend.th', 'them', 'contrast',\n",
       "       'signific', 'sourc', 'shift', 'hohner', 'draw', 'daylight',\n",
       "       'firmwar', 'templat', 'ten', 'bassist', 'lifeless', 'sens',\n",
       "       'unhappi', 'rubberi', 'eras', 'straight', 'levi', 'outsid',\n",
       "       'snake', 'primari', 'syndrom', 'attent', 'cent', 'doubt', 'favor',\n",
       "       'fx', 'im', '@', 'decis', 'zero', 'polici', 'p-bass', 'troubl',\n",
       "       'upgrad', 'glue', 'ub', 'add', 'lubric', 'howev', 'fourth',\n",
       "       'student', 'machin', 'force.w', 'mp3', 'various', 'depot',\n",
       "       'unnecessari', 'promis', 'shipment', 'nickel', 'petro', 'there\\\\',\n",
       "       'static', 'fault', 'joint', 'toy', 'pure', 'paper', 'bunch',\n",
       "       'skinni', 'blink', 'chorus', 'hunk', 'distrupt', 'event', 'sort',\n",
       "       'gift', 'complet', 'warehous', 'harder', 'trust', 'experiment',\n",
       "       'tripod', 'anybodi', 'instruct', 'appar', 'nasti', 'difficulti',\n",
       "       'normal', 'break', \"b'they\", 'technolog', 'sticki', 'or',\n",
       "       'flawless', 'do', 'sign', 'afraid', 'research', 'dont', 'soap',\n",
       "       'code', 'uncomfort', 'document', 'ta', 'refus', 'user', 'that',\n",
       "       'soooo', 'capacitor', 'portion', 'pass', 'faulti', 'visibl',\n",
       "       'condit', 'advis', 'question', 'plain', 'address', 'imag', 'slip',\n",
       "       'cut', 'highest', 'corner', 'anniversari', 'latest', 'wood',\n",
       "       '60th', 'closest', 'air', 'pedalboard', 'appear', 'socket',\n",
       "       'jackson', 'purpos', 'loss', 'unfair', 'imo', 'nobodi', 'author',\n",
       "       'numer'], dtype='<U15')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_arr[idx[1,:] < idx[0,:]]   #vocab for neg class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most words are associated with a negative feeling which aligns with our assumption that a 1 star reviews is negative. Some words listed above include nasti, cheaply, break, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "We can rank these words by their decreasing prior probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "thetaarray = nf_fit.theta.toArray().T\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "dtype = [('label', 'S10'), ('neg', float), ('pos', float)]\n",
    "prob_values = [ (vocab[i],\n",
    "                 np.exp(thetaarray[i,0])*(1-np.exp(thetaarray[i,1])),\n",
    "                 (1-np.exp(thetaarray[i,0]))*np.exp(thetaarray[i,1]))\n",
    "               for i in range(vocab_size) ]\n",
    "\n",
    "a = np.array(prob_values, dtype=dtype)       # create a structured array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('guitar', 0.006668331316865159, 0.008949923326246001)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([(b'pedal', 5.10275341e-03, 0.01302378),\n",
       "       (b'string', 4.18879502e-03, 0.01078995),\n",
       "       (b'guitar', 6.66833132e-03, 0.00894992),\n",
       "       (b'great', 1.64973165e-03, 0.00762815),\n",
       "       (b'amp', 7.55513350e-03, 0.00623695),\n",
       "       (b'case', 2.40428010e-03, 0.00614058),\n",
       "       (b'sound', 5.01402974e-03, 0.00588801),\n",
       "       (b'pick', 2.61484237e-03, 0.00564948),\n",
       "       (b'price', 2.28746169e-03, 0.00538022),\n",
       "       (b'joyo', 5.63118321e-04, 0.00494086),\n",
       "       (b'tone', 2.20021242e-03, 0.00481678),\n",
       "       (b'tuner', 3.53516182e-03, 0.00476615),\n",
       "       (b'other', 5.04294263e-03, 0.00476033),\n",
       "       (b'good', 3.49316226e-03, 0.00474767),\n",
       "       (b'differ', 1.56750224e-03, 0.0045025 ),\n",
       "       (b'stand', 5.74731133e-03, 0.00434902),\n",
       "       (b'more', 3.37425667e-03, 0.00418864),\n",
       "       (b'easi', 7.33212339e-04, 0.00398313),\n",
       "       (b'microphon', 9.24316288e-04, 0.0039802 ),\n",
       "       (b'overdr', 6.26869052e-05, 0.0039709 )],\n",
       "      dtype=[('label', 'S10'), ('neg', '<f8'), ('pos', '<f8')])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sort(a, order='pos')[::-1][0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([(b'amp', 0.00755513, 6.23695382e-03),\n",
       "       (b'guitar', 0.00666833, 8.94992333e-03),\n",
       "       (b'strap', 0.00611594, 3.86792398e-03),\n",
       "       (b'stand', 0.00574731, 4.34901810e-03),\n",
       "       (b'problem', 0.0053888 , 1.74449215e-03),\n",
       "       (b'product', 0.00520243, 1.96740529e-03),\n",
       "       (b'pedal', 0.00510275, 1.30237764e-02),\n",
       "       (b'other', 0.00504294, 4.76033418e-03),\n",
       "       (b'sound', 0.00501403, 5.88800899e-03),\n",
       "       (b'cabl', 0.00460681, 3.66733539e-03),\n",
       "       (b'thing', 0.00441564, 2.48073296e-03),\n",
       "       (b'issu', 0.00439096, 1.59803926e-03),\n",
       "       (b'cheap', 0.00426314, 1.23743653e-03),\n",
       "       (b'string', 0.0041888 , 1.07899492e-02),\n",
       "       (b'i\\\\', 0.00399233, 3.69778244e-03),\n",
       "       (b'few', 0.00398753, 1.95065656e-03),\n",
       "       (b'bad', 0.00390939, 5.30020555e-04),\n",
       "       (b'time', 0.00387453, 3.20540758e-03),\n",
       "       (b'way', 0.00380267, 1.50586247e-03),\n",
       "       (b'seller', 0.00372379, 8.08231106e-05)],\n",
       "      dtype=[('label', 'S10'), ('neg', '<f8'), ('pos', '<f8')])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sort(a, order='neg')[::-1][0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe some words that clearly carry out a positive/negative feeling. But they are mixed with other words that are only related to the products. It's because we have run this analysis on a dataset based on Instruments only. Thus, the positive/negative concept it biased by the terms related to the products people generally evaluate positively (or negatively)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
